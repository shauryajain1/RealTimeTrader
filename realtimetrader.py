# -*- coding: utf-8 -*-
"""RealTimeTrader.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yo82lONA2z_zCCwUM-_0Rz8F0AoxI32y

IMPORTING LIBRARIES
"""

pip install numpy pandas scikit-learn tensorflow yfinance ta-lib requests alpha_vantage alpaca-trade-api nltk flask docker prometheus_client grafana

"""DATA COLLECTION USING MULTIPLE APIs"""

import yfinance as yf
import pandas as pd
#from alpha_vantage.timeseries import TimeSeries
from alpaca_trade_api.rest import REST, TimeFrame

# Initialize APIs
av_api_key = 'YOUR_ALPHA_VANTAGE_KEY'
ts = TimeSeries(key=av_api_key, output_format='pandas')
alpaca_api = REST('YOUR_ALPACA_API_KEY', 'YOUR_ALPACA_SECRET_KEY', base_url='https://paper-api.alpaca.markets')

# Collect data for multiple stocks
tickers = ["AAPL", "MSFT", "GOOGL", "AMZN", "TSLA", ...]  # List of 10,000 tickers

all_data = {}
for ticker in tickers:
    data, meta_data = ts.get_daily(symbol=ticker, outputsize='full')
    all_data[ticker] = data

# Combine data into a single DataFrame
full_data = pd.concat(all_data, axis=1)
full_data.to_csv('all_stocks_data.csv')

"""REAL TIME DATA INTEGRATION"""

# Get real-time data for a specific stock
real_time_data = alpaca_api.get_barset('AAPL', TimeFrame.Minute, limit=100).df

# Include real-time data in model predictions

"""ADVANCED FEATURE ENGINEEERING"""

import numpy as np
import ta
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Feature Engineering
def add_features(df):
    df['MA50'] = df['close'].rolling(window=50).mean()
    df['MA200'] = df['close'].rolling(window=200).mean()
    df['RSI'] = ta.momentum.rsi(df['close'], window=14)
    df['MACD'] = ta.trend.macd_diff(df['close'])
    return df

# Apply features to all data
for ticker in all_data.keys():
    all_data[ticker] = add_features(all_data[ticker])

# Sentiment Analysis using NLP
nltk.download('vader_lexicon')
sia = SentimentIntensityAnalyzer()

def sentiment_analysis(news_headlines):
    sentiments = [sia.polarity_scores(headline)['compound'] for headline in news_headlines]
    return np.mean(sentiments)

# Assume you have a list of news headlines
news_headlines = ["Apple announces new product", "Google faces antitrust lawsuit", ...]
sentiment_score = sentiment_analysis(news_headlines)

# Add sentiment as a feature
for ticker in all_data.keys():
    all_data[ticker]['Sentiment'] = sentiment_score

"""ADVANCED MODEL ARCHITECTURE"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, LSTM, GRU, Dropout, Input, concatenate

# LSTM & GRU Model
def create_model(input_shape):
    inputs = Input(shape=input_shape)
    lstm_out = LSTM(50, return_sequences=True)(inputs)
    lstm_out = Dropout(0.2)(lstm_out)
    lstm_out = LSTM(50)(lstm_out)

    gru_out = GRU(50, return_sequences=True)(inputs)
    gru_out = Dropout(0.2)(gru_out)
    gru_out = GRU(50)(gru_out)

    combined = concatenate([lstm_out, gru_out])

    dense_out = Dense(25, activation='relu')(combined)
    outputs = Dense(1)(dense_out)

    model = Model(inputs, outputs)
    model.compile(optimizer='adam', loss='mean_squared_error')

    return model

model = create_model((50, 5))

"""BACKTESTING AND LIVE TRADING"""

import backtrader as bt

class MyStrategy(bt.Strategy):
    def __init__(self):
        self.ma50 = bt.indicators.SimpleMovingAverage(self.data.close, period=50)
        self.ma200 = bt.indicators.SimpleMovingAverage(self.data.close, period=200)

    def next(self):
        if self.ma50[0] > self.ma200[0] and not self.position:
            self.buy()
        elif self.ma50[0] < self.ma200[0] and self.position:
            self.sell()

cerebro = bt.Cerebro()
cerebro.addstrategy(MyStrategy)
cerebro.adddata(bt.feeds.PandasData(dataname=full_data))
cerebro.run()

"""RISK MANAGEMENT AND PORTFOLIO OPTIMIZATION"""

import numpy as np

def calculate_var(portfolio, confidence_level=0.95):
    mean_return = np.mean(portfolio)
    std_dev = np.std(portfolio)
    var = np.percentile(portfolio, (1-confidence_level) * 100)
    return var

portfolio = np.random.normal(0.001, 0.02, 1000)
var = calculate_var(portfolio)

print(f'Value at Risk: {var}')

# Portfolio Optimization using Markowitzâ€™s Efficient Frontier
from pypfopt.efficient_frontier import EfficientFrontier
from pypfopt import risk_models, expected_returns

mu = expected_returns.mean_historical_return(full_data)
S = risk_models.sample_cov(full_data)

ef = EfficientFrontier(mu, S)
weights = ef.max_sharpe()
cleaned_weights = ef.clean_weights()
ef.portfolio_performance(verbose=True)

"""DOCKER DEPLOYMENT"""

# Dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY . .

RUN pip install -r requirements.txt

CMD ["python", "app.py"]

"""FLASK API"""

from flask import Flask, request, jsonify
import numpy as np

app = Flask(__name__)

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()
    sequence = np.array(data['sequence']).reshape(1, 50, 5)
    prediction = model.predict(sequence)
    predicted_price = scaler.inverse_transform(prediction)
    return jsonify({'predicted_price': predicted_price[0][0]})

if __name__ == '__main__':
    app.run(debug=True)

"""MONITORING USING PROMETHEUS AND GRAFANA"""

pip install prometheus_client

from prometheus_client import start_http_server, Summary

REQUEST_TIME = Summary('request_processing_seconds', 'Time spent processing request')

@app.route('/predict', methods=['POST'])
@REQUEST_TIME.time()
def predict():
    ...